{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\credit_line_eligibility\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# For displaying all of the columns in dataframes\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#For ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df0 = pd.read_csv(r\"C:\\Users\\hp\\OneDrive\\Documents\\GitHub\\credit_line_eligibility\\data\\cleaned_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>dti</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>mort_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>11.44</td>\n",
       "      <td>10</td>\n",
       "      <td>141326</td>\n",
       "      <td>1.202703</td>\n",
       "      <td>117413</td>\n",
       "      <td>1</td>\n",
       "      <td>2094</td>\n",
       "      <td>1.089146</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.434536</td>\n",
       "      <td>41.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>11.99</td>\n",
       "      <td>4</td>\n",
       "      <td>173740</td>\n",
       "      <td>0.060161</td>\n",
       "      <td>117413</td>\n",
       "      <td>1</td>\n",
       "      <td>207128</td>\n",
       "      <td>0.623256</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681703</td>\n",
       "      <td>53.3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15600.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.49</td>\n",
       "      <td>0</td>\n",
       "      <td>141326</td>\n",
       "      <td>-0.796125</td>\n",
       "      <td>117893</td>\n",
       "      <td>1</td>\n",
       "      <td>73637</td>\n",
       "      <td>-0.513208</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079328</td>\n",
       "      <td>92.2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7200.0</td>\n",
       "      <td>36</td>\n",
       "      <td>6.49</td>\n",
       "      <td>6</td>\n",
       "      <td>141326</td>\n",
       "      <td>-0.319423</td>\n",
       "      <td>117413</td>\n",
       "      <td>1</td>\n",
       "      <td>73637</td>\n",
       "      <td>-2.120210</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.739714</td>\n",
       "      <td>21.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24375.0</td>\n",
       "      <td>60</td>\n",
       "      <td>17.27</td>\n",
       "      <td>9</td>\n",
       "      <td>173740</td>\n",
       "      <td>-0.281432</td>\n",
       "      <td>111005</td>\n",
       "      <td>0</td>\n",
       "      <td>73637</td>\n",
       "      <td>1.893119</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.927930</td>\n",
       "      <td>69.8</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt  term  int_rate  emp_length  home_ownership  annual_inc  \\\n",
       "0    10000.0    36     11.44          10          141326    1.202703   \n",
       "1     8000.0    36     11.99           4          173740    0.060161   \n",
       "2    15600.0    36     10.49           0          141326   -0.796125   \n",
       "3     7200.0    36      6.49           6          141326   -0.319423   \n",
       "4    24375.0    60     17.27           9          173740   -0.281432   \n",
       "\n",
       "   verification_status  loan_status  purpose       dti  open_acc  pub_rec  \\\n",
       "0               117413            1     2094  1.089146      16.0      0.0   \n",
       "1               117413            1   207128  0.623256      17.0      0.0   \n",
       "2               117893            1    73637 -0.513208      13.0      0.0   \n",
       "3               117413            1    73637 -2.120210       6.0      0.0   \n",
       "4               111005            0    73637  1.893119      13.0      0.0   \n",
       "\n",
       "   revol_bal  revol_util  total_acc  mort_acc  \n",
       "0   1.434536        41.8       25.0       0.0  \n",
       "1   0.681703        53.3       27.0       3.0  \n",
       "2   0.079328        92.2       26.0       0.0  \n",
       "3  -0.739714        21.5       13.0       0.0  \n",
       "4   0.927930        69.8       43.0       1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346311, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df0.drop(columns=['loan_status'])\n",
    "X.reset_index(inplace=True, drop=True)\n",
    "y = df0['loan_status']\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 1: Split data before standardization\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit & transform on training data\n",
    "X_test_scaled = scaler.transform(X_test)  # Only transform test data (NO fitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({1: 224616, 0: 52432})\n",
      "After SMOTE: Counter({0: 224616, 1: 224616})\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Apply SMOTE on the standardized training set\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Step 4: Print class distributions\n",
    "print(\"Before SMOTE:\", Counter(y_train))  \n",
    "print(\"After SMOTE:\", Counter(y_train_resampled)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_test_resampled, y_test_resampled = smote.fit_resample(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution: [224616 224616]\n",
      "Test class distribution: [56155 56155]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Train class distribution:\", np.bincount(y_train_resampled))  # Train data target class distribution fter SMOTE\n",
    "print(\"Test class distribution:\", np.bincount(y_test_resampled))  # Test data target class distribution after SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-17 18:49:54,905] A new study created in memory with name: no-name-65a847b2-e413-499b-a67e-924dd0c77d59\n",
      "[I 2025-03-17 18:50:17,536] Trial 0 finished with value: 0.7241242634686512 and parameters: {'n_estimators': 400, 'max_depth': 6, 'min_samples_split': 2, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.7241242634686512.\n",
      "[I 2025-03-17 18:50:37,314] Trial 1 finished with value: 0.7915476014721432 and parameters: {'n_estimators': 250, 'max_depth': 15, 'min_samples_split': 6, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:50:54,838] Trial 2 finished with value: 0.781662777789692 and parameters: {'n_estimators': 150, 'max_depth': 13, 'min_samples_split': 6, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:51:15,784] Trial 3 finished with value: 0.7838363320552346 and parameters: {'n_estimators': 200, 'max_depth': 13, 'min_samples_split': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:51:40,654] Trial 4 finished with value: 0.7875966747751413 and parameters: {'n_estimators': 250, 'max_depth': 14, 'min_samples_split': 6, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:51:56,541] Trial 5 finished with value: 0.6972782990342733 and parameters: {'n_estimators': 350, 'max_depth': 4, 'min_samples_split': 10, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:52:22,219] Trial 6 finished with value: 0.7779991525830722 and parameters: {'n_estimators': 250, 'max_depth': 12, 'min_samples_split': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:52:33,629] Trial 7 finished with value: 0.7636224106322407 and parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 10, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:52:59,329] Trial 8 finished with value: 0.7492227789915116 and parameters: {'n_estimators': 350, 'max_depth': 8, 'min_samples_split': 3, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:53:08,898] Trial 9 finished with value: 0.6766155228037394 and parameters: {'n_estimators': 350, 'max_depth': 3, 'min_samples_split': 9, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:53:12,886] Trial 10 finished with value: 0.7872009716873669 and parameters: {'n_estimators': 50, 'max_depth': 15, 'min_samples_split': 8, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:53:32,229] Trial 11 finished with value: 0.7915476014721432 and parameters: {'n_estimators': 250, 'max_depth': 15, 'min_samples_split': 6, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:53:44,671] Trial 12 finished with value: 0.7733928695692186 and parameters: {'n_estimators': 250, 'max_depth': 11, 'min_samples_split': 5, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:53:56,783] Trial 13 finished with value: 0.7925861382779495 and parameters: {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 7, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:54:05,635] Trial 14 finished with value: 0.7476779870157946 and parameters: {'n_estimators': 150, 'max_depth': 8, 'min_samples_split': 8, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:54:21,059] Trial 15 finished with value: 0.7918155011607724 and parameters: {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:54:32,791] Trial 16 finished with value: 0.7713724863933585 and parameters: {'n_estimators': 150, 'max_depth': 11, 'min_samples_split': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:54:44,758] Trial 17 finished with value: 0.784180684746782 and parameters: {'n_estimators': 200, 'max_depth': 13, 'min_samples_split': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:54:48,806] Trial 18 finished with value: 0.7551534644568679 and parameters: {'n_estimators': 50, 'max_depth': 9, 'min_samples_split': 8, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:55:00,798] Trial 19 finished with value: 0.7254745298844152 and parameters: {'n_estimators': 300, 'max_depth': 6, 'min_samples_split': 7, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:55:08,311] Trial 20 finished with value: 0.7848309161072629 and parameters: {'n_estimators': 100, 'max_depth': 14, 'min_samples_split': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:55:24,565] Trial 21 finished with value: 0.7919278419107537 and parameters: {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 5, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:55:46,224] Trial 22 finished with value: 0.7893269670536007 and parameters: {'n_estimators': 200, 'max_depth': 14, 'min_samples_split': 5, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:55:59,256] Trial 23 finished with value: 0.7777070464206434 and parameters: {'n_estimators': 150, 'max_depth': 12, 'min_samples_split': 5, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:56:38,483] Trial 24 finished with value: 0.7929674295377108 and parameters: {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 7, 'max_features': 'log2'}. Best is trial 24 with value: 0.7929674295377108.\n",
      "[I 2025-03-17 18:57:04,568] Trial 25 finished with value: 0.7790171321065169 and parameters: {'n_estimators': 300, 'max_depth': 12, 'min_samples_split': 7, 'max_features': 'log2'}. Best is trial 24 with value: 0.7929674295377108.\n",
      "[I 2025-03-17 18:57:29,937] Trial 26 finished with value: 0.7887015214275925 and parameters: {'n_estimators': 300, 'max_depth': 14, 'min_samples_split': 7, 'max_features': 'sqrt'}. Best is trial 24 with value: 0.7929674295377108.\n",
      "[I 2025-03-17 18:57:52,497] Trial 27 finished with value: 0.7829685258332806 and parameters: {'n_estimators': 300, 'max_depth': 13, 'min_samples_split': 7, 'max_features': 'log2'}. Best is trial 24 with value: 0.7929674295377108.\n",
      "[I 2025-03-17 18:58:32,683] Trial 28 finished with value: 0.79154451206801 and parameters: {'n_estimators': 400, 'max_depth': 15, 'min_samples_split': 9, 'max_features': 'log2'}. Best is trial 24 with value: 0.7929674295377108.\n",
      "[I 2025-03-17 18:58:39,883] Trial 29 finished with value: 0.7712780480091739 and parameters: {'n_estimators': 100, 'max_depth': 11, 'min_samples_split': 5, 'max_features': 'sqrt'}. Best is trial 24 with value: 0.7929674295377108.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1-Score: 0.7929674295377108\n",
      "Best Hyperparameters: {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 7, 'max_features': 'log2'}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Reduce dataset size for trials\n",
    "X_train_sample, y_train_sample = resample(X_train_resampled, y_train_resampled, n_samples=70000, random_state=42, stratify=y_train_resampled)\n",
    "\n",
    "rf_X_train, rf_X_val, rf_y_train, rf_y_val = train_test_split(\n",
    "    X_train_sample, y_train_sample, test_size=0.3, random_state=42, stratify=y_train_sample)\n",
    "\n",
    "# Define Optuna objective function\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 400, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    # min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    # ccp_alpha = trial.suggest_int(\"ccp_alpha\", 0.01, 0.1)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"])\n",
    "\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        criterion=\"gini\",\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        ccp_alpha=0,\n",
    "        max_features=max_features,\n",
    "        # min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Cross-validation with pruning\n",
    "    cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    score = cross_val_score(rf, rf_X_train, rf_y_train, \n",
    "                            cv=cv, scoring=\"f1\").mean()\n",
    "\n",
    "    return score\n",
    "\n",
    "# Use TPESampler for faster trials\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=30)\n",
    "print(\"Best F1-Score:\", study.best_value)\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8022\n",
      "Precision: 0.7785\n",
      "Recall: 0.8447\n",
      "F1-score: 0.8102\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79     56155\n",
      "           1       0.78      0.84      0.81     56155\n",
      "\n",
      "    accuracy                           0.80    112310\n",
      "   macro avg       0.80      0.80      0.80    112310\n",
      "weighted avg       0.80      0.80      0.80    112310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train RF with best parameters\n",
    "rf_best_params = study.best_params\n",
    "rf_best = RandomForestClassifier(class_weight=\"balanced\",**rf_best_params, random_state=42, n_jobs=-1)\n",
    "\n",
    "rf = rf_best.fit(X_train_resampled, y_train_resampled)\n",
    "joblib.dump(rf, 'rf_model.pkl')\n",
    "\n",
    "# # Evaluate on validation set\n",
    "y_pred = rf_best.predict(X_test_resampled)\n",
    "rf_preds = rf_best.predict_proba(rf_X_val)[:, 1]\n",
    "\n",
    "# Step 3: Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
    "precision = precision_score(y_test_resampled, y_pred, average=\"binary\")  # Use \"macro\" for multi-class\n",
    "recall = recall_score(y_test_resampled, y_pred, average=\"binary\")  # Use \"macro\" for multi-class\n",
    "f1 = f1_score(y_test_resampled, y_pred, average=\"binary\")  # Use \"macro\" for multi-class\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Full classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_resampled, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-17 23:06:29,596] A new study created in memory with name: no-name-c923feec-2f33-4287-b8af-c6042f05eedc\n",
      "[I 2025-03-17 23:06:35,382] Trial 0 finished with value: 0.8654285714285714 and parameters: {'learning_rate': 0.2114729622921267, 'max_depth': 10, 'min_child_weight': 7, 'gamma': 0.052083058687922665, 'subsample': 0.766224522035279, 'colsample_bytree': 0.5166934576430975, 'lambda': 0.21238877544971269, 'alpha': 0.9943149138302296}. Best is trial 0 with value: 0.8654285714285714.\n",
      "[I 2025-03-17 23:06:51,522] Trial 1 finished with value: 0.8665714285714285 and parameters: {'learning_rate': 0.025984259057600816, 'max_depth': 9, 'min_child_weight': 2, 'gamma': 0.07082640533601654, 'subsample': 0.6628063512271276, 'colsample_bytree': 0.9734993191701268, 'lambda': 3.2459732353526616, 'alpha': 0.9862393883645287}. Best is trial 1 with value: 0.8665714285714285.\n",
      "[I 2025-03-17 23:06:55,486] Trial 2 finished with value: 0.8661904761904762 and parameters: {'learning_rate': 0.2682570181509811, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.01179341531982444, 'subsample': 0.6509833679240915, 'colsample_bytree': 0.6148426026638736, 'lambda': 3.2325787092887674, 'alpha': 1.1223092499480483}. Best is trial 1 with value: 0.8665714285714285.\n",
      "[I 2025-03-17 23:07:02,055] Trial 3 finished with value: 0.8570952380952381 and parameters: {'learning_rate': 0.05595079635150472, 'max_depth': 3, 'min_child_weight': 1, 'gamma': 0.003751590512921994, 'subsample': 0.9431872666628609, 'colsample_bytree': 0.9720506838526278, 'lambda': 3.786780306722455, 'alpha': 0.7856661402656796}. Best is trial 1 with value: 0.8665714285714285.\n",
      "[I 2025-03-17 23:07:08,046] Trial 4 finished with value: 0.8365714285714285 and parameters: {'learning_rate': 0.02286664227857822, 'max_depth': 4, 'min_child_weight': 5, 'gamma': 0.012468421723088622, 'subsample': 0.5915096595790087, 'colsample_bytree': 0.6786802686674182, 'lambda': 5.6219784626424625, 'alpha': 0.01293336283713587}. Best is trial 1 with value: 0.8665714285714285.\n",
      "[I 2025-03-17 23:07:08,400] Trial 5 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:07:19,390] Trial 6 finished with value: 0.8584285714285714 and parameters: {'learning_rate': 0.02013793924401692, 'max_depth': 7, 'min_child_weight': 1, 'gamma': 0.22589368294693132, 'subsample': 0.9766856810432949, 'colsample_bytree': 0.5816052457741527, 'lambda': 0.7611305560843391, 'alpha': 0.10921251706774765}. Best is trial 1 with value: 0.8665714285714285.\n",
      "[I 2025-03-17 23:07:19,653] Trial 7 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:07:19,935] Trial 8 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:07:20,318] Trial 9 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:07:37,609] Trial 10 finished with value: 0.8425238095238096 and parameters: {'learning_rate': 0.010927525333109272, 'max_depth': 10, 'min_child_weight': 5, 'gamma': 0.001190627036431628, 'subsample': 0.5089151478704689, 'colsample_bytree': 0.8463072041017881, 'lambda': 0.0028351927052289922, 'alpha': 8.642341726660275}. Best is trial 1 with value: 0.8665714285714285.\n",
      "[I 2025-03-17 23:07:37,981] Trial 11 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:07:38,363] Trial 12 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:07:46,781] Trial 13 finished with value: 0.8172380952380952 and parameters: {'learning_rate': 0.013002606512236566, 'max_depth': 5, 'min_child_weight': 3, 'gamma': 0.12481447138281879, 'subsample': 0.6700165123073791, 'colsample_bytree': 0.8587984473338284, 'lambda': 0.9953263317652545, 'alpha': 4.212385827403157}. Best is trial 1 with value: 0.8665714285714285.\n",
      "[I 2025-03-17 23:07:47,441] Trial 14 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:07:48,073] Trial 15 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:07:54,848] Trial 16 finished with value: 0.8500952380952381 and parameters: {'learning_rate': 0.02497575064815314, 'max_depth': 5, 'min_child_weight': 6, 'gamma': 0.018927636976992145, 'subsample': 0.7079780875052442, 'colsample_bytree': 0.6050254322168838, 'lambda': 9.834387289201553, 'alpha': 0.2633080220224624}. Best is trial 1 with value: 0.8665714285714285.\n",
      "[I 2025-03-17 23:08:08,168] Trial 17 finished with value: 0.8593809523809524 and parameters: {'learning_rate': 0.016767538423371094, 'max_depth': 8, 'min_child_weight': 2, 'gamma': 0.0019980174679302843, 'subsample': 0.5282668736231793, 'colsample_bytree': 0.9043299356543246, 'lambda': 0.2969647209510955, 'alpha': 0.042443164037165804}. Best is trial 1 with value: 0.8665714285714285.\n",
      "[I 2025-03-17 23:08:08,524] Trial 18 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:08:08,855] Trial 19 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:08:09,276] Trial 20 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:08:09,841] Trial 21 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:08:10,658] Trial 22 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:08:11,128] Trial 23 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:08:11,501] Trial 24 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:08:11,823] Trial 25 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:08:12,148] Trial 26 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:08:12,423] Trial 27 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:08:12,815] Trial 28 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 23:08:13,108] Trial 29 pruned. Trial was pruned at iteration 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.8665714285714285\n",
      "Best Parameters: {'learning_rate': 0.025984259057600816, 'max_depth': 9, 'min_child_weight': 2, 'gamma': 0.07082640533601654, 'subsample': 0.6628063512271276, 'colsample_bytree': 0.9734993191701268, 'lambda': 3.2459732353526616, 'alpha': 0.9862393883645287}\n"
     ]
    }
   ],
   "source": [
    "from optuna.integration import XGBoostPruningCallback\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Step 1: Create a fixed validation set (50K rows)\n",
    "X_train_sample, y_train_sample = resample(X_train_resampled, y_train_resampled, n_samples=70000, random_state=42, stratify=y_train_resampled)\n",
    "\n",
    "xgb_X_train, X_val, xgb_y_train, y_val = train_test_split(\n",
    "    X_train_sample, y_train_sample, test_size=0.3, random_state=42, stratify=y_train_sample)\n",
    "\n",
    "# Step 2: Define the Optuna objective function\n",
    "def objective(trial):\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"tree_method\": \"gpu_hist\",  # GPU optimization\n",
    "        \"verbosity\":0,\n",
    "        \"verbose\":-1,\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-3, 1.0),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-3, 10.0),\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-3, 10.0),\n",
    "        \"n_estimators\": 500,  # High number for early stopping\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"early_stopping_rounds\":20,\n",
    "        \"callbacks\":[XGBoostPruningCallback(trial, \"validation_0-logloss\")],\n",
    "    }\n",
    "    \n",
    "    # pruning_callback = XGBoostPruningCallback(trial, \"validation_0-logloss\")\n",
    "    # Train the model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(xgb_X_train, xgb_y_train, eval_set=[(X_val, y_val)], verbose=False )\n",
    "\n",
    "    # Predict on validation set\n",
    "    preds = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "    return accuracy\n",
    "\n",
    "# Step 3: Run Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Accuracy:\", study.best_value)\n",
    "print(\"Best Parameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8233\n",
      "Precision: 0.7976\n",
      "Recall: 0.8665\n",
      "F1-score: 0.8306\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.82     56155\n",
      "           1       0.80      0.87      0.83     56155\n",
      "\n",
      "    accuracy                           0.82    112310\n",
      "   macro avg       0.83      0.82      0.82    112310\n",
      "weighted avg       0.83      0.82      0.82    112310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train final model using best params on full training data\n",
    "xgb_best_params = study.best_params\n",
    "\n",
    "xgb_best = xgb.XGBClassifier(**xgb_best_params)\n",
    "xgb=xgb_best.fit(X_train_resampled, y_train_resampled)\n",
    "joblib.dump(xgb, 'xgb_model.pkl')\n",
    "\n",
    "y_pred = xgb_best.predict(X_test_resampled)\n",
    "xgb_pred = xgb_best.predict_proba(X_val)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
    "precision = precision_score(y_test_resampled, y_pred)\n",
    "recall = recall_score(y_test_resampled, y_pred)\n",
    "f1 = f1_score(y_test_resampled, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Full classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_resampled, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ROC-AUC Score: 0.9287084535147392\n",
      "Best Parameters: {'learning_rate': 0.1386185612536086, 'num_leaves': 260, 'max_depth': 13, 'min_data_in_leaf': 122, 'lambda_l1': 0.109871274254064, 'lambda_l2': 0.003365798596021806, 'feature_fraction': 0.46518892933626377, 'bagging_fraction': 0.829746096276152, 'bagging_freq': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from optuna.pruners import HyperbandPruner\n",
    "\n",
    "# Step 1: Create a fixed validation set (50K rows)\n",
    "X_train_sample, y_train_sample = resample(X_train_resampled, y_train_resampled, n_samples=70000, random_state=42, stratify=y_train_resampled)\n",
    "\n",
    "lgb_X_train, lgb_X_val, lgb_y_train, lgb_y_val = train_test_split(\n",
    "    X_train_sample, y_train_sample, test_size=0.3, random_state=42, stratify=y_train_sample)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    param_grid = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"verbose\":-1,\n",
    "        \"boosting_type\": 'gbdt', \n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 400, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 200),\n",
    "        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-3, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-3, 10.0),\n",
    "        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(lgb_X_train, label=lgb_y_train)\n",
    "    val_data = lgb.Dataset(lgb_X_val, label=lgb_y_val)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        param_grid,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=[\"train\", \"valid_0\"],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)],\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(lgb_X_val)\n",
    "    return roc_auc_score(lgb_y_val, preds)\n",
    "    \n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=HyperbandPruner)\n",
    "study.optimize(objective, n_trials=20)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best ROC-AUC Score:\", study.best_value)\n",
    "print(\"Best Parameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8798\n",
      "Precision: 0.8203\n",
      "Recall: 0.9728\n",
      "F1-score: 0.8900\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.79      0.87     56155\n",
      "           1       0.82      0.97      0.89     56155\n",
      "\n",
      "    accuracy                           0.88    112310\n",
      "   macro avg       0.89      0.88      0.88    112310\n",
      "weighted avg       0.89      0.88      0.88    112310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train final LightGBM model with best parameters\n",
    "lgb_best_params = study.best_params\n",
    "lgb_best = lgb.LGBMClassifier(**lgb_best_params)\n",
    "lgb_best.fit(X_train_resampled, y_train_resampled)\n",
    "joblib.dump(lgb_best, 'lgbm_model.pkl')\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = lgb_best.predict(X_test_resampled)\n",
    "lgb_preds = lgb_best.predict_proba(lgb_X_val)[:, 1]\n",
    "\n",
    "# Evaluate results\n",
    "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
    "precision = precision_score(y_test_resampled, y_pred)\n",
    "recall = recall_score(y_test_resampled, y_pred)\n",
    "f1 = f1_score(y_test_resampled, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_resampled, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
