{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\credit_line_eligibility\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# For displaying all of the columns in dataframes\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#For ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df0 = pd.read_csv(r\"C:\\Users\\hp\\OneDrive\\Documents\\GitHub\\credit_line_eligibility\\data\\cleaned_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>dti</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>mort_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>11.44</td>\n",
       "      <td>10</td>\n",
       "      <td>141326</td>\n",
       "      <td>1.202703</td>\n",
       "      <td>117413</td>\n",
       "      <td>1</td>\n",
       "      <td>2094</td>\n",
       "      <td>1.089146</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.434536</td>\n",
       "      <td>41.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>11.99</td>\n",
       "      <td>4</td>\n",
       "      <td>173740</td>\n",
       "      <td>0.060161</td>\n",
       "      <td>117413</td>\n",
       "      <td>1</td>\n",
       "      <td>207128</td>\n",
       "      <td>0.623256</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681703</td>\n",
       "      <td>53.3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15600.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.49</td>\n",
       "      <td>0</td>\n",
       "      <td>141326</td>\n",
       "      <td>-0.796125</td>\n",
       "      <td>117893</td>\n",
       "      <td>1</td>\n",
       "      <td>73637</td>\n",
       "      <td>-0.513208</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079328</td>\n",
       "      <td>92.2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7200.0</td>\n",
       "      <td>36</td>\n",
       "      <td>6.49</td>\n",
       "      <td>6</td>\n",
       "      <td>141326</td>\n",
       "      <td>-0.319423</td>\n",
       "      <td>117413</td>\n",
       "      <td>1</td>\n",
       "      <td>73637</td>\n",
       "      <td>-2.120210</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.739714</td>\n",
       "      <td>21.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24375.0</td>\n",
       "      <td>60</td>\n",
       "      <td>17.27</td>\n",
       "      <td>9</td>\n",
       "      <td>173740</td>\n",
       "      <td>-0.281432</td>\n",
       "      <td>111005</td>\n",
       "      <td>0</td>\n",
       "      <td>73637</td>\n",
       "      <td>1.893119</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.927930</td>\n",
       "      <td>69.8</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt  term  int_rate  emp_length  home_ownership  annual_inc  \\\n",
       "0    10000.0    36     11.44          10          141326    1.202703   \n",
       "1     8000.0    36     11.99           4          173740    0.060161   \n",
       "2    15600.0    36     10.49           0          141326   -0.796125   \n",
       "3     7200.0    36      6.49           6          141326   -0.319423   \n",
       "4    24375.0    60     17.27           9          173740   -0.281432   \n",
       "\n",
       "   verification_status  loan_status  purpose       dti  open_acc  pub_rec  \\\n",
       "0               117413            1     2094  1.089146      16.0      0.0   \n",
       "1               117413            1   207128  0.623256      17.0      0.0   \n",
       "2               117893            1    73637 -0.513208      13.0      0.0   \n",
       "3               117413            1    73637 -2.120210       6.0      0.0   \n",
       "4               111005            0    73637  1.893119      13.0      0.0   \n",
       "\n",
       "   revol_bal  revol_util  total_acc  mort_acc  \n",
       "0   1.434536        41.8       25.0       0.0  \n",
       "1   0.681703        53.3       27.0       3.0  \n",
       "2   0.079328        92.2       26.0       0.0  \n",
       "3  -0.739714        21.5       13.0       0.0  \n",
       "4   0.927930        69.8       43.0       1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346311, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df0.drop(columns=['loan_status'])\n",
    "X.reset_index(inplace=True, drop=True)\n",
    "y = df0['loan_status']\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 1: Split data before standardization\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit & transform on training data\n",
    "X_test_scaled = scaler.transform(X_test)  # Only transform test data (NO fitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({1: 224616, 0: 52432})\n",
      "After SMOTE: Counter({0: 224616, 1: 224616})\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Apply SMOTE on the standardized training set\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Step 4: Print class distributions\n",
    "print(\"Before SMOTE:\", Counter(y_train))  \n",
    "print(\"After SMOTE:\", Counter(y_train_resampled)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_test_resampled, y_test_resampled = smote.fit_resample(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution: [224616 224616]\n",
      "Test class distribution: [56155 56155]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Train class distribution:\", np.bincount(y_train_resampled))  # Train data target class distribution fter SMOTE\n",
    "print(\"Test class distribution:\", np.bincount(y_test_resampled))  # Test data target class distribution after SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-17 18:49:54,905] A new study created in memory with name: no-name-65a847b2-e413-499b-a67e-924dd0c77d59\n",
      "[I 2025-03-17 18:50:17,536] Trial 0 finished with value: 0.7241242634686512 and parameters: {'n_estimators': 400, 'max_depth': 6, 'min_samples_split': 2, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.7241242634686512.\n",
      "[I 2025-03-17 18:50:37,314] Trial 1 finished with value: 0.7915476014721432 and parameters: {'n_estimators': 250, 'max_depth': 15, 'min_samples_split': 6, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:50:54,838] Trial 2 finished with value: 0.781662777789692 and parameters: {'n_estimators': 150, 'max_depth': 13, 'min_samples_split': 6, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:51:15,784] Trial 3 finished with value: 0.7838363320552346 and parameters: {'n_estimators': 200, 'max_depth': 13, 'min_samples_split': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:51:40,654] Trial 4 finished with value: 0.7875966747751413 and parameters: {'n_estimators': 250, 'max_depth': 14, 'min_samples_split': 6, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:51:56,541] Trial 5 finished with value: 0.6972782990342733 and parameters: {'n_estimators': 350, 'max_depth': 4, 'min_samples_split': 10, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:52:22,219] Trial 6 finished with value: 0.7779991525830722 and parameters: {'n_estimators': 250, 'max_depth': 12, 'min_samples_split': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:52:33,629] Trial 7 finished with value: 0.7636224106322407 and parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 10, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:52:59,329] Trial 8 finished with value: 0.7492227789915116 and parameters: {'n_estimators': 350, 'max_depth': 8, 'min_samples_split': 3, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:53:08,898] Trial 9 finished with value: 0.6766155228037394 and parameters: {'n_estimators': 350, 'max_depth': 3, 'min_samples_split': 9, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:53:12,886] Trial 10 finished with value: 0.7872009716873669 and parameters: {'n_estimators': 50, 'max_depth': 15, 'min_samples_split': 8, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:53:32,229] Trial 11 finished with value: 0.7915476014721432 and parameters: {'n_estimators': 250, 'max_depth': 15, 'min_samples_split': 6, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:53:44,671] Trial 12 finished with value: 0.7733928695692186 and parameters: {'n_estimators': 250, 'max_depth': 11, 'min_samples_split': 5, 'max_features': 'log2'}. Best is trial 1 with value: 0.7915476014721432.\n",
      "[I 2025-03-17 18:53:56,783] Trial 13 finished with value: 0.7925861382779495 and parameters: {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 7, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:54:05,635] Trial 14 finished with value: 0.7476779870157946 and parameters: {'n_estimators': 150, 'max_depth': 8, 'min_samples_split': 8, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:54:21,059] Trial 15 finished with value: 0.7918155011607724 and parameters: {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:54:32,791] Trial 16 finished with value: 0.7713724863933585 and parameters: {'n_estimators': 150, 'max_depth': 11, 'min_samples_split': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:54:44,758] Trial 17 finished with value: 0.784180684746782 and parameters: {'n_estimators': 200, 'max_depth': 13, 'min_samples_split': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:54:48,806] Trial 18 finished with value: 0.7551534644568679 and parameters: {'n_estimators': 50, 'max_depth': 9, 'min_samples_split': 8, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:55:00,798] Trial 19 finished with value: 0.7254745298844152 and parameters: {'n_estimators': 300, 'max_depth': 6, 'min_samples_split': 7, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:55:08,311] Trial 20 finished with value: 0.7848309161072629 and parameters: {'n_estimators': 100, 'max_depth': 14, 'min_samples_split': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:55:24,565] Trial 21 finished with value: 0.7919278419107537 and parameters: {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 5, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:55:46,224] Trial 22 finished with value: 0.7893269670536007 and parameters: {'n_estimators': 200, 'max_depth': 14, 'min_samples_split': 5, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:55:59,256] Trial 23 finished with value: 0.7777070464206434 and parameters: {'n_estimators': 150, 'max_depth': 12, 'min_samples_split': 5, 'max_features': 'log2'}. Best is trial 13 with value: 0.7925861382779495.\n",
      "[I 2025-03-17 18:56:38,483] Trial 24 finished with value: 0.7929674295377108 and parameters: {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 7, 'max_features': 'log2'}. Best is trial 24 with value: 0.7929674295377108.\n",
      "[I 2025-03-17 18:57:04,568] Trial 25 finished with value: 0.7790171321065169 and parameters: {'n_estimators': 300, 'max_depth': 12, 'min_samples_split': 7, 'max_features': 'log2'}. Best is trial 24 with value: 0.7929674295377108.\n",
      "[I 2025-03-17 18:57:29,937] Trial 26 finished with value: 0.7887015214275925 and parameters: {'n_estimators': 300, 'max_depth': 14, 'min_samples_split': 7, 'max_features': 'sqrt'}. Best is trial 24 with value: 0.7929674295377108.\n",
      "[I 2025-03-17 18:57:52,497] Trial 27 finished with value: 0.7829685258332806 and parameters: {'n_estimators': 300, 'max_depth': 13, 'min_samples_split': 7, 'max_features': 'log2'}. Best is trial 24 with value: 0.7929674295377108.\n",
      "[I 2025-03-17 18:58:32,683] Trial 28 finished with value: 0.79154451206801 and parameters: {'n_estimators': 400, 'max_depth': 15, 'min_samples_split': 9, 'max_features': 'log2'}. Best is trial 24 with value: 0.7929674295377108.\n",
      "[I 2025-03-17 18:58:39,883] Trial 29 finished with value: 0.7712780480091739 and parameters: {'n_estimators': 100, 'max_depth': 11, 'min_samples_split': 5, 'max_features': 'sqrt'}. Best is trial 24 with value: 0.7929674295377108.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1-Score: 0.7929674295377108\n",
      "Best Hyperparameters: {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 7, 'max_features': 'log2'}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Reduce dataset size for trials\n",
    "X_train_sample, y_train_sample = resample(X_train_resampled, y_train_resampled, n_samples=70000, random_state=42, stratify=y_train_resampled)\n",
    "\n",
    "rf_X_train, rf_X_val, rf_y_train, rf_y_val = train_test_split(\n",
    "    X_train_sample, y_train_sample, test_size=0.3, random_state=42, stratify=y_train_sample)\n",
    "\n",
    "# Define Optuna objective function\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 400, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    # min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    # ccp_alpha = trial.suggest_int(\"ccp_alpha\", 0.01, 0.1)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"])\n",
    "\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        criterion=\"gini\",\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        ccp_alpha=0,\n",
    "        max_features=max_features,\n",
    "        # min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Cross-validation with pruning\n",
    "    cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    score = cross_val_score(rf, rf_X_train, rf_y_train, \n",
    "                            cv=cv, scoring=\"f1\").mean()\n",
    "\n",
    "    return score\n",
    "\n",
    "# Use TPESampler for faster trials\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=30)\n",
    "print(\"Best F1-Score:\", study.best_value)\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8022\n",
      "Precision: 0.7785\n",
      "Recall: 0.8447\n",
      "F1-score: 0.8102\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79     56155\n",
      "           1       0.78      0.84      0.81     56155\n",
      "\n",
      "    accuracy                           0.80    112310\n",
      "   macro avg       0.80      0.80      0.80    112310\n",
      "weighted avg       0.80      0.80      0.80    112310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train RF with best parameters\n",
    "rf_best_params = study.best_params\n",
    "rf_best = RandomForestClassifier(class_weight=\"balanced\",**rf_best_params, random_state=42, n_jobs=-1)\n",
    "\n",
    "rf = rf_best.fit(X_train_resampled, y_train_resampled)\n",
    "joblib.dump(rf, 'rf_model.pkl')\n",
    "\n",
    "# # Evaluate on validation set\n",
    "y_pred = rf_best.predict(X_test_resampled)\n",
    "rf_preds = rf_best.predict_proba(rf_X_val)[:, 1]\n",
    "\n",
    "# Step 3: Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
    "precision = precision_score(y_test_resampled, y_pred, average=\"binary\")  # Use \"macro\" for multi-class\n",
    "recall = recall_score(y_test_resampled, y_pred, average=\"binary\")  # Use \"macro\" for multi-class\n",
    "f1 = f1_score(y_test_resampled, y_pred, average=\"binary\")  # Use \"macro\" for multi-class\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Full classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_resampled, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449232, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449232,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-17 02:12:04,564] A new study created in memory with name: no-name-2edec595-1157-4e45-b985-a137e23f7395\n",
      "[I 2025-03-17 02:12:12,629] Trial 0 finished with value: 0.8661333333333333 and parameters: {'learning_rate': 0.028933336025850177, 'max_depth': 7, 'min_child_weight': 7, 'gamma': 0.004334347278173934, 'subsample': 0.5968127960422207, 'colsample_bytree': 0.9762145025499056, 'lambda': 0.039945988595207524, 'alpha': 0.003986516988356314}. Best is trial 0 with value: 0.8661333333333333.\n",
      "[I 2025-03-17 02:12:19,718] Trial 1 finished with value: 0.8710666666666667 and parameters: {'learning_rate': 0.07374945929636939, 'max_depth': 9, 'min_child_weight': 9, 'gamma': 0.01247504906835454, 'subsample': 0.8887360211354072, 'colsample_bytree': 0.9291942903300328, 'lambda': 0.12595287878881797, 'alpha': 0.014605250785984198}. Best is trial 1 with value: 0.8710666666666667.\n",
      "[I 2025-03-17 02:12:28,093] Trial 2 finished with value: 0.8694 and parameters: {'learning_rate': 0.05943126672822237, 'max_depth': 7, 'min_child_weight': 1, 'gamma': 0.021954983998377968, 'subsample': 0.5995500504097266, 'colsample_bytree': 0.7061110457906734, 'lambda': 0.0039655828409642865, 'alpha': 0.7988193775013375}. Best is trial 1 with value: 0.8710666666666667.\n",
      "[I 2025-03-17 02:12:30,759] Trial 3 finished with value: 0.8654666666666667 and parameters: {'learning_rate': 0.28995087367714883, 'max_depth': 5, 'min_child_weight': 2, 'gamma': 0.0010906055900468611, 'subsample': 0.7396373253898889, 'colsample_bytree': 0.9509888584077579, 'lambda': 0.01813184961585505, 'alpha': 0.00513539343337903}. Best is trial 1 with value: 0.8710666666666667.\n",
      "[I 2025-03-17 02:12:32,707] Trial 4 finished with value: 0.861 and parameters: {'learning_rate': 0.23700463161266694, 'max_depth': 6, 'min_child_weight': 5, 'gamma': 0.045895660867201, 'subsample': 0.5231513496513809, 'colsample_bytree': 0.817680998097598, 'lambda': 0.0011064963321837638, 'alpha': 0.005681113731043968}. Best is trial 1 with value: 0.8710666666666667.\n",
      "[I 2025-03-17 02:12:42,300] Trial 5 finished with value: 0.8648666666666667 and parameters: {'learning_rate': 0.028975542319825506, 'max_depth': 8, 'min_child_weight': 10, 'gamma': 0.012005537675710934, 'subsample': 0.6931896480026736, 'colsample_bytree': 0.9288029414300709, 'lambda': 3.583234839296288, 'alpha': 0.5102984016351685}. Best is trial 1 with value: 0.8710666666666667.\n",
      "[I 2025-03-17 02:12:51,316] Trial 6 finished with value: 0.8664666666666667 and parameters: {'learning_rate': 0.026923750392342134, 'max_depth': 7, 'min_child_weight': 1, 'gamma': 0.002350644308806452, 'subsample': 0.9143230304711796, 'colsample_bytree': 0.952521977337929, 'lambda': 0.6325690574166044, 'alpha': 0.0018941034577870974}. Best is trial 1 with value: 0.8710666666666667.\n",
      "[I 2025-03-17 02:13:02,677] Trial 7 finished with value: 0.8712 and parameters: {'learning_rate': 0.03645909660365636, 'max_depth': 10, 'min_child_weight': 8, 'gamma': 0.0023865400336785664, 'subsample': 0.9417076475248576, 'colsample_bytree': 0.5190744717617447, 'lambda': 3.372854403465213, 'alpha': 0.8000788489612762}. Best is trial 7 with value: 0.8712.\n",
      "[I 2025-03-17 02:13:02,902] Trial 8 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:03,129] Trial 9 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:07,628] Trial 10 finished with value: 0.7718 and parameters: {'learning_rate': 0.010729332583249311, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.08441080555964786, 'subsample': 0.8398948827669295, 'colsample_bytree': 0.5083750694315394, 'lambda': 9.415171989883197, 'alpha': 0.08069635282831687}. Best is trial 7 with value: 0.8712.\n",
      "[I 2025-03-17 02:13:08,010] Trial 11 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:08,344] Trial 12 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:19,732] Trial 13 finished with value: 0.8599333333333333 and parameters: {'learning_rate': 0.01473332086952576, 'max_depth': 9, 'min_child_weight': 7, 'gamma': 0.014203182584763947, 'subsample': 0.8324665213399661, 'colsample_bytree': 0.6103358552146797, 'lambda': 0.16499946626531195, 'alpha': 0.35603765893732975}. Best is trial 7 with value: 0.8712.\n",
      "[I 2025-03-17 02:13:20,038] Trial 14 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:20,364] Trial 15 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:32,582] Trial 16 finished with value: 0.866 and parameters: {'learning_rate': 0.01920611976273597, 'max_depth': 10, 'min_child_weight': 9, 'gamma': 0.30418089505986406, 'subsample': 0.8684786304901435, 'colsample_bytree': 0.5872622791308187, 'lambda': 0.05005178987053599, 'alpha': 1.4278820768477487}. Best is trial 7 with value: 0.8712.\n",
      "[I 2025-03-17 02:13:32,881] Trial 17 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:33,194] Trial 18 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:33,546] Trial 19 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:33,857] Trial 20 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:34,237] Trial 21 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:34,491] Trial 22 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:39,109] Trial 23 finished with value: 0.8522 and parameters: {'learning_rate': 0.04335602765393316, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 0.07047780517940982, 'subsample': 0.9619298473268459, 'colsample_bytree': 0.5684468421393544, 'lambda': 0.02470806313617705, 'alpha': 0.11475565200516476}. Best is trial 7 with value: 0.8712.\n",
      "[I 2025-03-17 02:13:39,340] Trial 24 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:48,950] Trial 25 finished with value: 0.8612666666666666 and parameters: {'learning_rate': 0.02222862633018983, 'max_depth': 8, 'min_child_weight': 4, 'gamma': 0.0035018725939588787, 'subsample': 0.5227537471340707, 'colsample_bytree': 0.6950255338039086, 'lambda': 4.352709156108235, 'alpha': 0.055895275736419996}. Best is trial 7 with value: 0.8712.\n",
      "[I 2025-03-17 02:13:49,269] Trial 26 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:49,517] Trial 27 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:49,908] Trial 28 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-17 02:13:50,201] Trial 29 pruned. Trial was pruned at iteration 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.03645909660365636, 'max_depth': 10, 'min_child_weight': 8, 'gamma': 0.0023865400336785664, 'subsample': 0.9417076475248576, 'colsample_bytree': 0.5190744717617447, 'lambda': 3.372854403465213, 'alpha': 0.8000788489612762}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Step 1: Create a fixed validation set (50K rows)\n",
    "X_train_sample, y_train_sample = resample(X_train_resampled, y_train_resampled, n_samples=50000, random_state=42, stratify=y_train_resampled)\n",
    "\n",
    "X_train_sample, X_val, y_train_sample, y_val = train_test_split(\n",
    "    X_train_sample, y_train_sample, test_size=0.3, random_state=42, stratify=y_train_sample)\n",
    "\n",
    "# Step 2: Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    # Sample 50K rows from training set for faster trials\n",
    "    # sample_idx = np.random.choice(len(X_train_subset), 50000, replace=False)\n",
    "    # X_sample, y_sample = X_train_subset.loc[sample_idx], y_train_subset.loc[sample_idx]\n",
    "\n",
    "    # sample_idx = np.random.choice(X_train_subset.index, 50000, replace=False)\n",
    "    # X_sample, y_sample = X_train_subset.loc[sample_idx], y_train_subset.loc[sample_idx]\n",
    "\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"tree_method\": \"gpu_hist\",  # GPU optimization\n",
    "        \"verbosity\":False,\n",
    "        \"verbose\":-1,\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-3, 1.0),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-3, 10.0),\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-3, 10.0),\n",
    "        \"n_estimators\": 500,  # High number for early stopping\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"early_stopping_rounds\":20,\n",
    "        \"callbacks\":[XGBoostPruningCallback(trial, \"validation_0-logloss\")],\n",
    "    }\n",
    "    \n",
    "    # pruning_callback = XGBoostPruningCallback(trial, \"validation_0-logloss\")\n",
    "    # Train the model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train_sample,\n",
    "        y_train_sample,\n",
    "        eval_set=[(X_val, y_val)],  # Fixed validation set\n",
    "        # early_stopping_rounds=20,\n",
    "        verbose=False,\n",
    "        # callbacks=[XGBoostPruningCallback(trial, \"validation_0-logloss\")],\n",
    "    )\n",
    "\n",
    "    # Predict on validation set\n",
    "    preds = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "    return accuracy\n",
    "\n",
    "# Step 3: Run Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8500\n",
      "Precision: 0.8082\n",
      "Recall: 0.9179\n",
      "F1-score: 0.8596\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.78      0.84     56155\n",
      "           1       0.81      0.92      0.86     56155\n",
      "\n",
      "    accuracy                           0.85    112310\n",
      "   macro avg       0.86      0.85      0.85    112310\n",
      "weighted avg       0.86      0.85      0.85    112310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train final model using best params on full training data\n",
    "best_params = study.best_params\n",
    "final_model = xgb.XGBClassifier(**best_params)\n",
    "final_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred = final_model.predict(X_test_resampled)\n",
    "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
    "precision = precision_score(y_test_resampled, y_pred)\n",
    "recall = recall_score(y_test_resampled, y_pred)\n",
    "f1 = f1_score(y_test_resampled, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Full classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_resampled, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1647139156131148, 'num_leaves': 80, 'max_depth': 9, 'min_data_in_leaf': 27, 'lambda_l1': 7.056780416992359, 'lambda_l2': 2.743192045079168e-07, 'feature_fraction': 0.9268769558547962, 'bagging_fraction': 0.9900576847529032, 'bagging_freq': 3}\n"
     ]
    }
   ],
   "source": [
    "from optuna.integration.lightgbm import LightGBMPruningCallback\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Step 1: Create a fixed validation set (50K rows)\n",
    "X_train_sample, y_train_sample = resample(X_train_resampled, y_train_resampled, n_samples=50000, random_state=42, stratify=y_train_resampled)\n",
    "\n",
    "X_train_sample, X_val, y_train_sample, y_val = train_test_split(\n",
    "    X_train_sample, y_train_sample, test_size=0.3, random_state=42, stratify=y_train_sample)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    param_grid = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"verbose\":-1,\n",
    "        \"boosting_type\": 'gbdt',  # Can also try 'dart'\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 300, step=10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 200),\n",
    "        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_sample, label=y_train_sample)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    # pruning_callback = \n",
    "    \n",
    "    model = lgb.train(\n",
    "        param_grid,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=[\"train\", \"valid_0\"],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)],\n",
    "        # verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_val)  # Keep raw probability scores\n",
    "    return roc_auc_score(y_val, preds)\n",
    "    \n",
    "\n",
    "from optuna.pruners import HyperbandPruner\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=HyperbandPruner)\n",
    "study.optimize(objective, n_trials=30)\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8796\n",
      "Precision: 0.8196\n",
      "Recall: 0.9735\n",
      "F1-score: 0.8899\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.79      0.87     56155\n",
      "           1       0.82      0.97      0.89     56155\n",
      "\n",
      "    accuracy                           0.88    112310\n",
      "   macro avg       0.89      0.88      0.88    112310\n",
      "weighted avg       0.89      0.88      0.88    112310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train final LightGBM model with best parameters\n",
    "best_params = study.best_params\n",
    "best_model = lgb.LGBMClassifier(**best_params)\n",
    "best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = best_model.predict(X_test_resampled)\n",
    "\n",
    "# Evaluate results\n",
    "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
    "precision = precision_score(y_test_resampled, y_pred)\n",
    "recall = recall_score(y_test_resampled, y_pred)\n",
    "f1 = f1_score(y_test_resampled, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_resampled, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
